{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.constraints import unitnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import random_uniform\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = kernel_size - 1\n",
    "    for i in range(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l+2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data(revs, word_idx_map, max_l=51, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, val, test = [], [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev['text'], word_idx_map, max_l, kernel_size)\n",
    "        sent.append(rev['y'])\n",
    "        if rev['split'] == 1:\n",
    "            train.append(sent)\n",
    "        elif rev['split'] == 0:\n",
    "            val.append(sent)\n",
    "    train = np.array(train, dtype=np.int)\n",
    "    val = np.array(val, dtype=np.int)\n",
    "    return [train, val]\n",
    "\n",
    "\n",
    "print (\"loading data...\")\n",
    "with open(\"imdb-train-val-testN.pickle\", 'rb') as f:\n",
    "    x = pickle.load(f, encoding='latin')\n",
    "revs, W, word_idx_map, vocab = x[0], x[1], x[2], x[3]\n",
    "print (\"data loaded!\")\n",
    "\n",
    "datasets = make_idx_data(revs, word_idx_map, max_l=2721,kernel_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put train data in separate NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape = (2209, 2729)\n",
      "train_Y.shape = (2209, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train data preparation\n",
    "N = datasets[0].shape[0]\n",
    "conv_input_width = W.shape[1]\n",
    "conv_input_height = int(datasets[0].shape[1]-1)\n",
    "\n",
    "# For each word write a word index (not vector) to X tensor\n",
    "train_X = np.zeros((N, conv_input_height), dtype=np.int)\n",
    "train_Y = np.zeros((N, 2), dtype=np.int)\n",
    "for i in range(N):\n",
    "    for j in range(conv_input_height):\n",
    "        train_X[i, j] = datasets[0][i, j]\n",
    "    \n",
    "print ('train_X.shape = {}'.format(train_X.shape))\n",
    "print ('train_Y.shape = {}'.format(train_Y.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_train = pd.read_csv('essays.csv',encoding = \"latin\")\n",
    "for i in range(N):\n",
    "    train_Y[i,data_train.iloc[i,3]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2209, 2729)\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put validation data in separate NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_X.shape = (258, 2729)\n",
      "val_Y.shape = (258, 2)\n"
     ]
    }
   ],
   "source": [
    "# Validation data preparation\n",
    "Nv = datasets[1].shape[0]\n",
    "\n",
    "# For each word write a word index (not vector) to X tensor\n",
    "val_X = np.zeros((Nv, conv_input_height), dtype=np.int)\n",
    "val_Y = np.zeros((Nv, 2), dtype=np.int)\n",
    "for i in range(Nv):\n",
    "    for j in range(conv_input_height):\n",
    "        val_X[i, j] = datasets[1][i, j]\n",
    "    \n",
    "print ('val_X.shape = {}'.format(val_X.shape))\n",
    "print ('val_Y.shape = {}'.format(val_Y.shape))\n",
    "for i in range(Nv):\n",
    "    val_Y[i,data_train.iloc[i,3]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TensorBoard(log_dir = 'logs/pp_cnn_d0.4_mse_200')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and compile CNN model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=30418, output_dim=300, input_length=2729, weights=[array([[ ..., name=\"e_l\", embeddings_constraint=<keras.con...)`\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(200, (5, 300), kernel_initializer=\"random_uniform\", padding=\"valid\", kernel_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "backend.set_image_dim_ordering('th')\n",
    "\n",
    "# Number of feature maps (outputs of convolutional layer)\n",
    "N_fm = 200\n",
    "# kernel size of convolutional layer\n",
    "kernel_size = 5\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding layer (lookup table of trainable word vectors)\n",
    "model.add(Embedding(input_dim=W.shape[0], \n",
    "                    output_dim=W.shape[1], \n",
    "                    input_length=conv_input_height,\n",
    "                    weights=[W], \n",
    "                    W_constraint=unitnorm(),\n",
    "                    name = 'e_l'))\n",
    "# Reshape word vectors from Embedding to tensor format suitable for Convolutional layer\n",
    "model.add(Reshape((1, conv_input_height, conv_input_width)))\n",
    "\n",
    "# first convolutional layer\n",
    "model.add(Convolution2D(N_fm,\n",
    "                        kernel_size, \n",
    "                        conv_input_width,\n",
    "                        kernel_initializer='random_uniform',\n",
    "                        border_mode='valid',\n",
    "                        W_regularizer=l2(0.001)))\n",
    "# ReLU activation\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# aggregate data in every feature map to scalar using MAX operation\n",
    "model.add(MaxPooling2D(pool_size=(conv_input_height-kernel_size+1,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128,kernel_initializer='random_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "# Inner Product layer (as in regular neural network, but without non-linear activation function)\n",
    "model.add(Dense(2))\n",
    "# SoftMax activation; actually, Dense+SoftMax works as Multinomial Logistic Regression\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Custom optimizers could be used, though right now standard adadelta is employed\n",
    "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None)\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for N_epoch epochs (could be run as many times as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2209 samples, validate on 258 samples\n",
      "Epoch 1/10\n",
      "2209/2209 [==============================] - 23s 11ms/step - loss: 0.3165 - acc: 0.4803 - val_loss: 0.2516 - val_acc: 0.5039\n",
      "Epoch 2/10\n",
      "2209/2209 [==============================] - 23s 10ms/step - loss: 0.2508 - acc: 0.4857 - val_loss: 0.2508 - val_acc: 0.4961\n",
      "Epoch 3/10\n",
      "2209/2209 [==============================] - 23s 10ms/step - loss: 0.2508 - acc: 0.4934 - val_loss: 0.2511 - val_acc: 0.4961\n",
      "Epoch 4/10\n",
      "2209/2209 [==============================] - 24s 11ms/step - loss: 0.2508 - acc: 0.4848 - val_loss: 0.2511 - val_acc: 0.5039\n",
      "Epoch 5/10\n",
      "2209/2209 [==============================] - 25s 11ms/step - loss: 0.2509 - acc: 0.5097 - val_loss: 0.2508 - val_acc: 0.5078\n",
      "Epoch 6/10\n",
      "2209/2209 [==============================] - 26s 12ms/step - loss: 0.2511 - acc: 0.4889 - val_loss: 0.2509 - val_acc: 0.4961\n",
      "Epoch 7/10\n",
      "2209/2209 [==============================] - 26s 12ms/step - loss: 0.2511 - acc: 0.4880 - val_loss: 0.2506 - val_acc: 0.4961\n",
      "Epoch 8/10\n",
      "2209/2209 [==============================] - 26s 12ms/step - loss: 0.2510 - acc: 0.5111 - val_loss: 0.2511 - val_acc: 0.5039\n",
      "Epoch 9/10\n",
      "2209/2209 [==============================] - 27s 12ms/step - loss: 0.2505 - acc: 0.5070 - val_loss: 0.2513 - val_acc: 0.5039\n",
      "Epoch 10/10\n",
      "2209/2209 [==============================] - 27s 12ms/step - loss: 0.2502 - acc: 0.5192 - val_loss: 0.2524 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177d45abeb8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, batch_size=32, epochs = 10, validation_data=(val_X,val_Y), verbose=1, callbacks = [tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/258 [==============================] - 1s 3ms/step\n",
      "acc: 51.55%\n",
      "51.55% (+/- 0.00%)\n"
     ]
    }
   ],
   "source": [
    "cvscores=[]\n",
    "scores = model.evaluate(val_X, val_Y, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_10epochsN.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put test data in separate NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X.shape = (468, 2729)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11622"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nt = datasets[2].shape[0]\n",
    "\n",
    "test_X = np.zeros((Nt, conv_input_height), dtype=np.int)\n",
    "for i in range(Nt):\n",
    "    for j in range(conv_input_height):\n",
    "       test_X[i, j] = datasets[2][i, j]\n",
    "    \n",
    "print ('test_X.shape = {}'.format(test_X.shape))\n",
    "test_X[5][232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(test_X, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
